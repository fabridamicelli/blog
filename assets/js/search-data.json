{
  
    
        "post0": {
            "title": "Achtung: Watch out, German csv readers!",
            "content": "TL;DR: pandas.read_csv considers the word &quot;null&quot; as a NaN, which also means &quot;zero&quot; in German. The arguments na_values and keep_default_na offer a solution. . It&#39;s Friday an you set out to build a very sophisticated numbers translator in several languages: . import pandas as pd numbers = pd.DataFrame({ &quot;Spanish&quot;: [&quot;cero&quot;, &quot;uno&quot;, &quot;dos&quot;, &quot;tres&quot;], &quot;English&quot;: [&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;], &quot;German&quot;: [&quot;null&quot;, &quot;eins&quot;, &quot;zwei&quot;, &quot;drei&quot;], }) numbers.index = numbers.index.rename(&quot;Number&quot;) numbers . Spanish English German . Number . 0 cero | zero | null | . 1 uno | one | eins | . 2 dos | two | zwei | . 3 tres | three | drei | . If you want to know how to say 3 in Spanish, you do: . numbers.loc[3, &quot;Spanish&quot;] . &#39;tres&#39; . Nice. You save the super advanced translator for later use and go off for a relaxed weekend. . numbers.to_csv(&quot;numbers&quot;) . Back to work on Monday, your German friend drops by your office and you want to proudly show what you&#39;ve created. So you load your &quot;translator&quot; and go like: – Ask me how to say any number! – OK. Let&#39;s start easy: how do you say zero in German? . &quot;That I can do&quot;, you think and type: . # Load the awesome translator numbers_loaded = pd.read_csv(&quot;numbers&quot;, index_col=&quot;Number&quot;) # And get the translation numbers_loaded.loc[0, &quot;German&quot;] . nan . Oh no, that&#39;s no good! You get out of the embarrassing situation saying it is actually a beta version, and, and, and. The harm is done and your friend leaves the office skeptical – to say the least. . What&#39;s was the problem? The answer is in the docstrings of the function pd.read_csv. If we look carefully at which values pandas considers as NaN per default we find the following: . &quot;&quot;&quot; na_values : scalar, str, list-like, or dict, optional Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: &#39;&#39;, &#39;#N/A&#39;, &#39;#N/A N/A&#39;, &#39;#NA&#39;, &#39;-1.#IND&#39;, &#39;-1.#QNAN&#39;, &#39;-NaN&#39;, &#39;-nan&#39;, &#39;1.#IND&#39;, &#39;1.#QNAN&#39;, &#39;N/A&#39;, &#39;NA&#39;, &#39;NULL&#39;, &#39;NaN&#39;, &#39;n/a&#39;, &#39;nan&#39;, &#39;null&#39;. &quot;&quot;&quot; . There it is: &quot;null&quot; is in the list! . Solution: We need to do two things: . Pass other values without &quot;null&quot; (and &quot;NULL&quot; if you&#39;re not sure everything is lowercase). | Tell pandas not to keep the defaults (otherwise it will use both the defaults and the passed values). | . na_values = [ &#39;&#39;, &#39;#N/A&#39;, &#39;#N/A N/A&#39;, &#39;#NA&#39;, &#39;-1.#IND&#39;, &#39;-1.#QNAN&#39;, &#39;-NaN&#39;, &#39;-nan&#39;, &#39;1.#IND&#39;, &#39;1.#QNAN&#39;, &#39;N/A&#39;, &#39;NA&#39;, &#39;NaN&#39;, &#39;n/a&#39;, &#39;nan&#39; ] numbers_loaded = pd.read_csv( &quot;numbers&quot;, index_col=&quot;Number&quot;, na_values=na_values, keep_default_na=False ) numbers_loaded . Spanish English German . Number . 0 cero | zero | null | . 1 uno | one | eins | . 2 dos | two | zwei | . 3 tres | three | drei | . Now we can ask: . numbers_loaded.loc[0, &quot;German&quot;] . &#39;null&#39; . That will keep your German friends happy :) . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail). .",
            "url": "https://fabridamicelli.github.io/blog/python/pandas/text/2020/05/20/achtung-german-csv-reader.html",
            "relUrl": "/python/pandas/text/2020/05/20/achtung-german-csv-reader.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Explicit is better than implicit",
            "content": "TL; DR: Only use the form array *= something if you&#39;re 100% sure you are doing the right thing, otherwise, just go for array = array * something. . Let&#39;s see why. We define two functions that to the eyes of many (including past me) do just the same. . import numpy as np def multiply(array, scalar): array *= scalar # &lt;-- handy short hand, right? ;) return array def multiply2(array, scalar): array = array * scalar return array . Let&#39;s see them in action . a = np.arange(10.) # dot casts to float to avoid type errors b = np.arange(10.) . a . array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) . b . array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) . multiply(a, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . multiply(a, 2) . array([ 0., 4., 8., 12., 16., 20., 24., 28., 32., 36.]) . Hey, wait! What&#39;s going on? . a . array([ 0., 4., 8., 12., 16., 20., 24., 28., 32., 36.]) . . Warning: The operation modifies the array in place. . Let&#39;s see what the other version of our function does. . multiply2(b, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . multiply2(b, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . This time the input array stays the same, ie., the modification remained in the scope of the function. . Despite it being very basic, it is actually more difficult to debug than for the toy example in real life cases. For instance, in the middle of a long data preprocessing pipeline. If you load your data once and run the preprocessing pipeline once, you will probably not notice the bug (that&#39;s the tricky thing!). But if the loaded data are passed more than once through the pipeline (without reloading the whole data), each pass will be actually feeding different input. For example, if you run K-Fold cross-validation, most likely it won&#39;t crash or anything, but you will be passing K different datasets to your model and your validation will be just rubbish! . Conclusions: . array *= something is very different from array = array * something | You&#39;d better be really sure of what you&#39;re doing with array = array * something. | . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail (fabridamicelli at gmail). .",
            "url": "https://fabridamicelli.github.io/blog/python/numpy/2020/05/10/numpy-array-in-place.html",
            "relUrl": "/python/numpy/2020/05/10/numpy-array-in-place.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Does your embedding make sense?",
            "content": "It&#39;s not about the projections for the rest of 2020, I promise. Nor 2021. . TL;DR: . Does your embedding make any (human) sense? Take a *quick, interactive look* at its labels (if you have) &amp; compare it w/clusters.Thanks to great, idiomatic Altair! @jakevdp @vega_vis -Code: https://t.co/vpWRU6fs6H-Example: https://t.co/67F7AuDoNr@PyData #Python @python_tip pic.twitter.com/PIeDuNZ2gf . &mdash; Fabrizio Damicelli (@fabridamicelli) December 12, 2019 . Imagine you are working with high-dimensional data, that is, the position of each data point in that multidimensional space can be represented by a large number of other features/coordinates. For example, you measure a bunch of properties of a product where each item has some values associated, say, size, cost of production, price, CO2 footprint, etc. It could also be the case that your features are more abstract quantities. Instead of price or size, you could just have a bunch of numbers representing each item that don&#39;t necessarily have a human meaning, for instance a vector like this [0.11, 0.34, 0.15, 0.9]. Hopefully, those number actually mean something (whatever we understand by &quot;meaning something&quot;). Hopefully. Things start to get trickier. A beautiful example of that are word-embeddings, which is the thing I was originally playing with when I needed the visualization that I will show you. These embeddings represent each word with a vector, typically having between 300 and 1000 dimensions. That&#39;s what I mean by high-dimensional. What the coordinates actually mean in that case is highly non-trivial and you can find out about it in the above-mentioned article. Other datasets are even more massive in terms of the number of features though, e.g., gene expression data are usually orders of magnitude larger. . I&#39;d be saying nothing new if I pointed out that humans (we typically don&#39;t talk about that with other animals) have a hard time visualizing space with more than three dimensions. Even with 86 billion of neurons between their ears. At this point, several of you will be saying: &quot;but! Carl Sagan... bla, bla&quot;.. In case you&#39;re too young, too old or simple had something else better to do and didn&#39;t watch that episode, just watch this short video and then get back – you won&#39;t regreat: . . Back to the data. People do all sorts of tricks to wrap their mind around those wild, high-dimensional data in spaces with really funky shapes and fundamentally counterintuitive to our daily spatial perception of the world. Here comes a whole zoo of dimensionality reduction methods (e.g., PCA, tSNE) that project your data down to something your visual system can deal with. Of course, visualization is just one use case, but you may also want to store your data more efficiently or find outliers, or simply make life easier to your machine learning algorithm. We won&#39;t talk about those here. . Why bother? The good, the bad and the ugly metric. . Let&#39;s say you have your projected data in 2D, for example, after tSNE. First things first: you might want to check if the projection that your method just spit out makes any sense at all. But, why, what can go wrong? Well, a lot. For instance, your dimensionality reduction method might be using an inappropiate metric. In other words, the algorithm could be using a notion of distance between points in the multidimensional space which does not capture some fundamental aspect of the funky space that those data points live in. Thus, further operations we do with/to our data might be strongly misleading. That is not what we want. To keep it concrete, just take a look at these nice examples from the documentation of the UMAP library, where the effect of using different metrics is clearly depicted. . Some libraries, like UMAP, also allow you to document your embedding or plot interactive figures. But you might have a tougher luck this time around and just get a cloud of unintelligible points scattered when you plot the 2D-projected data. In that case, you might find it useful to interact with it by looking at the labels of your data. The labels can be anything, like a priori known categories the result of your favourite clustering method. . That&#39;s what you came for :) So without further ado, on to the code. . #collapse-hide import altair as alt import pandas as pd from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.cluster import KMeans def plot_interactive_embedding( source=None, x=None, y=None, target_col=None, color_col=None, alpha=0.9, markersize=40, grid=True, max_n_show = 25, figsize=(500, 500), filename=None ): &quot;&quot;&quot; Simple function for interactive visualization labels of a 2D embedding (e.g., PCA projection). Altair Chart is generated (as html), where one can brush over the scatter plots and given labels are shown. Color can be optionally mapped to values as well (e.g., to compare embedding with a clustering method). This is a quick adaptation of this example: https://altair-viz.github.io/gallery/scatter_linked_table.html Parameters - source: pandas Dataframe Data to plot. x: str Column name of x coordinate data. This name will be also used as axis label. y: str Column name of y coordinate data. This name will be also used as axis label. target_col: str Column name of target data, i.e., the labels to brush over. color_col: str, optional. Default None. Column name of data encoding color. If None, all points will have same color. alpha: float (0-1), optional. Default .9. Opacity of points. markersize: float, int, optional. Default 40. Size of the points. grid: bool, optional. Default True. Grid in the background. Set to False to remove it. max_n_show: int, optional. Dafault 25. Maximum number of (target) labels to show when brushing over the points. figsize: tuple (floats), optional. Default (500, 500). Values for (width, height) filename: str, optional. Default None. If given, the chart will be saved. The name must include extension - one of [.json, .html, .svg, .png]. Returns - chart: Altair Chart Instance of chart for further tweaking &quot;&quot;&quot; width, height = figsize # Brush for selection brush = alt.selection(type=&#39;interval&#39;) # Scatter Plot points = alt.Chart( source, width=width, height=height ).mark_circle(size=markersize).encode( x=x, y=y, color=alt.value(&#39;steelblue&#39;) if color_col is None else alt.Color(color_col+&quot;:N&quot;, scale=alt.Scale(scheme=&#39;Spectral&#39;)) ).add_selection(brush) # Base chart for data tables ranked_text = alt.Chart(source).mark_text().encode( y=alt.Y(&#39;row_number:O&#39;,axis=None) ).transform_window( row_number=&#39;row_number()&#39; ).transform_filter( brush ).transform_window( rank=&#39;rank(row_number)&#39; ).transform_filter( alt.datum.rank &lt; max_n_show ) # Data Tables text = ranked_text.encode(text=target_col+&quot;:N&quot;).properties(title=target_col) chart = alt.hconcat( points, text, ).configure_axis(grid=grid) if filename: chart.save(filename) return chart . . #collapse-show # Get digits dataset digits = load_digits() # Cluster it with kmeans and get the predicted labels kmlabel = KMeans(n_clusters=10, n_jobs=-1).fit_predict(digits.data) # Embed in 2D with tSNE embedding = TSNE(n_components=2, n_iter=500).fit_transform(digits.data) # Turn data into a dataframe digits_df = pd.DataFrame({ &quot;tSNE-1&quot;: embedding[:, 0], &quot;tSNE-2&quot;: embedding[:, 1], &quot;digit&quot;: digits.target, &quot;KM-label&quot;: kmlabel }) # Make the chart plot_interactive_embedding( source=digits_df, x=&quot;tSNE-1&quot;, y=&quot;tSNE-2&quot;, target_col=&quot;digit&quot;, color_col=&quot;KM-label&quot;, figsize=(450, 450), filename=&quot;your_filename&quot; ) . . You can save the output by passing a filename argument. That will generate an interactive file like this. . All the code can be found here. . A couple of links worth checking out: . Altair: &quot;Declarative Visualization in Python&quot;. This does the job under the hood of what I showed you. I did not even scratch the surface of that powerful, neat library. | This and this for cool visualizations of UMAP projections. | whatlies: &quot;A library that tries help you to understand. What lies in word embeddings?&quot;. This didn&#39;t exist when I wrote this function – would be my first choice now. It goes way beyond of what I showed and has integration with other NLP tools as well. | . Bonus: Funny games in high dimensions! . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "relUrl": "/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "date": " • Dec 12, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Divide and conquer",
            "content": "TL; DR: If you need to compute many vector pairwise metrics in batches, try sklearn.metrics.pairwise_distances_chunked . The problem . I had to compute pairwise cosine distances for a large list of high-dimensional vectors (e.g. word embedding). After a couple of (very bad) possible solutions I found a reasonable one, of course, standing on the shoulders of giants: the sklearn function sklearn.metrics.pairwise_distances_chunked. It is pretty much a one-liner and you don&#39;t need to care about manually splitting/parallelizing things. This is a quick write-up for other people to save that time. . The intuition behind the computation we want to achieve is depicted in the following plot: . . Two vectors in 2D space represented as points. The blue line shows the euclidean distance between the vectors. The $ cos( alpha)$ is the cosine distance. . #collapse-hide # Import everything we need import time import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.spatial.distance import cosine from sklearn.metrics.pairwise import cosine_distances from sklearn.metrics import pairwise_distances_chunked from tqdm import tqdm import seaborn as sns sns.set(context=&quot;notebook&quot;, font_scale=1.4, style=&quot;whitegrid&quot;) . . For the sake of presentation and simplicity, we are just going to create and use a bunch of random vectors. And that bunch is going to be relatively large (to compute on a not-so-large laptop). . Let&#39;s first get an idea of how many values need to compute if we take say 50 thousand vectors, which is not unrealistic at all (e.g., that could be taking all nouns of a word embedding): . n_vectors = 50_000 print(&quot;Number of pairwise distances to be computed&quot;, int(n_vectors * (n_vectors-1) / 2)) . Number of pairwise distances to be computed 1249975000 . Oh, that is indeed quite a few of them. . The default solution . The most straight forward to do this is with scipy/sklearn pairwise distances functions as follows (we are going to time it to get an idea of how the problem scales). . %%time n_vectors = 5_000 n_dimensions = 300 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) # Compute pairwise distances with function from sklearn distances = cosine_distances(all_vectors) . CPU times: user 1.53 s, sys: 347 ms, total: 1.88 s Wall time: 286 ms . So far, so good. But what happens if we want to compute it for more vectors? . %%time n_vectors = 15_000 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) distances = cosine_distances(all_vectors) . CPU times: user 5.75 s, sys: 3.22 s, total: 8.97 s Wall time: 1.99 s . As we see, we have 3 times more vectors, but the computation takes ~9 times longer! The reason for that is that the complexity of the problem scales non-linearly with the size of the input (number of vectors). In particular, the complexity is $O(n^2)$, since we are filling the upper half of a square matrix, which grows as the square of number of vectors divided by two (which does not count for the limit case). . In practice, we don&#39;t care much about those calculations as long as our computer is able to manage it in a reasonable time. . So how about having more vectors, say 30 or 50 thousand? . Closer look at the scaling . Let&#39;s examine the computational complexity we mentioned above with some quick numerical experiments. . #collapse-hide def evaluate_scaling(func, n_vectors_vals, n_dimensions=300): &quot;&quot;&quot;Return times of func execution for n_vectors_vals&quot;&quot;&quot; times = [] for n_vectors in tqdm(n_vectors_vals): all_vectors = np.random.random(size=(n_vectors, n_dimensions)) st = time.time() distances = func(all_vectors) times.append(time.time()-st) del distances return times def plot_scaling(n_vectors_vals, times, **kwargs): plt.plot(n_vectors_vals, times, linewidth=3, alpha=.8, **kwargs) plt.xlabel(&quot;Number of vectors&quot;) plt.ylabel(&quot;Time elapsed (seconds)&quot;) plt.grid(True, linestyle=&quot;--&quot;, alpha=0.7) . . n_vectors_vals = np.arange(1000, 20001, 500) times = evaluate_scaling(cosine_distances, n_vectors_vals) . 100%|██████████| 39/39 [00:49&lt;00:00, 1.27s/it] . plot_scaling(n_vectors_vals, times) . We can use what we know about the complexity ($O(n^2)$) to fit a curve. In other words, we are going to fit a quadratic function that predicts the time it takes to compute all the pairwise distances as a function of the number of vectors. After that, we can use that function to extrapolate and estimate the performance for a much larger number of vectors. . . # Fit a 2nd degree polynomial and get the polynomial evaluator fit = np.polyfit(n_vectors_vals, times, 2) poly = np.poly1d(fit) # Check our fit plot_scaling(n_vectors_vals, times, label=&quot;Actual&quot;) plot_scaling(n_vectors_vals, poly(n_vectors_vals), label=&quot;Quadratic Fit&quot;) plt.legend(); . The fit looks approximately correct. Remember, we don&#39;t need to get a perfectly accurate estimate. We rather want to know if we should grab a coffee while the computation runs, let it compute overnight or if it is unfeasible with our hardware. . Now we extrapolate for more vectors: . n_vectors_large = np.arange(1000, 50000) plot_scaling(n_vectors_large, poly(n_vectors_large)) . Well that doesn&#39;t sound too bad: it should take around 40 seconds to compute the distances for 50 thousand vectors. Let&#39;s give it a try: . n_vectors = 50_000 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) distances = cosine_distances(all_vectors) . [I 15:19:07.791 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports kernel 4055ff16-c49b-4a76-9068-e899d001fb85 restarted . Ups! We&#39;ve blown up the memory and forced the kernel to commit suicide. If you&#39;re running the code along and 50 thousand still works in your computer, just try a higher number, you&#39;ll get there pretty soon. My machine is fine until ~30 thousand vectors. . In short, what we thought was our initial problem (computation time) is actually secondary (it would take less than a minute). But either the result itself (cosine distances matrix) or other structures during intermediate computations simply don&#39;t fit in memory. . Solution: first attempt . Iterate and compute the values one by one instead of computing it with the cosine_distances function. Spoiler: Bad idea. Let&#39;s see: . #collapse-hide def cosine_distances_iter(all_vectors): n_vectors = all_vectors.shape[0] distances = np.zeros((n_vectors, n_vectors)) # D is symmetric, so we don&#39;t want to compute twice - just use upper diag indices for i, j in zip(*np.triu_indices(n_vectors, k=1)): distances[i, j] = cosine(all_vectors[i], all_vectors[j]) return distances . . n_vectors = 100 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) distances = cosine_distances_iter(all_vectors) . As the distance matrix is symmetric, we don&#39;t repeat the computation and thus here we just show the upper triangle. Each entry $D_{ij}$ of the distance matrix corresponds to the cosine distance between the vectors $i$ and $j$. . #collapse-hide def plot_distances_heatmap(distances): sns.heatmap( distances, mask=~np.triu(distances).astype(bool), cbar_kws={&quot;label&quot;: &quot;Cosine distance&quot;}, cmap=&quot;magma&quot;, square=True, xticklabels=False, yticklabels=False, ) plt.title(&quot;Distance Matrix&quot;) plt.show() . . plot_distances_heatmap(distances) . Now, how does it scale? We can do the same curve fitting as above and project for a larger number of vectors. . #collapse-hide n_vectors_vals = np.arange(100, 1001, 100) times = evaluate_scaling(cosine_distances_iter, n_vectors_vals) # Fit a 2nd degree polynomial and get the polynomial evaluator fit = np.polyfit(n_vectors_vals, times, 2) poly = np.poly1d(fit) # Check our fit plot_scaling(n_vectors_vals, times, label=&quot;Actual&quot;) plot_scaling(n_vectors_vals, poly(n_vectors_vals), label=&quot;Quadratic Fit&quot;) plt.legend(); . . 100%|██████████| 10/10 [01:03&lt;00:00, 6.36s/it] . And the extrapolation looks like this: . #collapse-hide n_vectors_large = np.arange(100, 50001, 100) plot_scaling(n_vectors_large, poly(n_vectors_large)) . . That is going to take a while – way longer than grabbing a coffee. Conclusion: there must be a better way! . Same giants, same shoulders: scikit-learn to the rescue . A much better alternative was to look into the scikit-learn library. It turns out there is a function pairwise_distances_chunked, which does exactly what we want. As the documentation explains, this function creates a Python generator that will build up a distance matrix chunk by chunk, thus computing the distances as lazily and returning the intermediate results. The following example will be hopefully useful: . n_vectors = 20_000 n_dimensions = 100 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) # We create an empty placeholder for the results, so that we # can visualize the intermediate steps distances = np.zeros((n_vectors, n_vectors)) . Instead of computing and storing all the results, we construct the generator first: . chunk_generator = pairwise_distances_chunked(all_vectors, metric=&quot;cosine&quot;) type(chunk_generator) . generator . Now we can call the next method and so generate the first chunk of results. Like with any other Python generator, we can repeat that call until the generator is exhausted. . chunk1 = next(chunk_generator) print(&quot;Shape of chunk 1:&quot;, chunk1.shape) chunk2 = next(chunk_generator) print(&quot;Shape of chunk 2:&quot;, chunk2.shape) chunk3 = next(chunk_generator) print(&quot;Shape of chunk 3:&quot;, chunk3.shape) print(&quot;Total size along first dimension :&quot;, sum((chunk1.shape[0], chunk2.shape[0], chunk3.shape[0]))) . Shape of chunk 1: (6710, 20000) Shape of chunk 2: (6710, 20000) Shape of chunk 3: (6580, 20000) Total size along first dimension : 20000 . As we observe on the shape of the generated results, the rendered chunk is a vertical slice of the complete distance matrix. We can visualize it (yellow corresponds to the computed values): . #collapse-hide # WARNING: running this cell might take quite a bit of memory chunk_generator = pairwise_distances_chunked(all_vectors, metric=&quot;cosine&quot;) fig, axes = plt.subplots(ncols=3, figsize=(15, 4)) chunks_idx = range(1, 4) # this depends on the number of total chunks (which I happen to know is 3 here) current_row = 0 distances = np.zeros((n_vectors, n_vectors)) for ax, c in zip(axes.flat, chunks_idx): chunk = next(chunk_generator) n_rows, _ = chunk.shape # Update distances matrix distances[current_row: current_row + n_rows, :] = chunk current_row += n_rows ax.imshow(distances, cmap=&quot;RdYlBu_r&quot;) ax.set_title(f&quot;Distance Matrix after chunk{c}&quot;, fontsize=15) ax.grid(False) ax.set_xticks([]) ax.set_yticks([]) . . . Note: The size of each chunk will be figured out automatically by scikit-learn, no need to worry about that. If the results are small enough, it might just dispatch it all in one batch. . Back to 50K . So far so good. But our original problem was substantially larger than the example above, namely 50 (not 20) thousand vectors, which we already saw translates into many more computations. . Now we are going to test the chunked approach with more vectors – that&#39;s what you came for :) . n_vectors = 50_000 n_dimensions = 300 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) chunk_generator = pairwise_distances_chunked(all_vectors, metric=&quot;cosine&quot;) . Assuming the final whole array fits in memory, we could collect all chunks and then concatenate them, like this: . distances = np.vstack(chunk_generator) . which is very nice, but will stop working with numpy 1.16, thus we need another container . # This might take a lot of RAM, so depending on your hardware you might just skip the concatenation distances = np.vstack([chunk for chunk in chunk_generator]) print(&quot;distances shape: &quot;, distances.shape) . distances shape: (50000, 50000) . Voilà! We just computed the pairwise cosine distance for the 50 thousand vectors! If your matrix distance is too big such that cannot be concatenated into one array, then you can simply do whatever you need to with the individual chunks and save the intermediate results. . For the sake of completeness, let&#39;s evaluate the scaling of that function: . #collapse-hide def cosine_distance_chunks(all_vectors): chunk_generator = pairwise_distances_chunked(all_vectors, metric=&quot;cosine&quot;) return np.vstack([chunk for chunk in chunk_generator]) . . n_vectors_vals = np.arange(10_000, 50_001, 10_000) times = evaluate_scaling(cosine_distance_chunks, n_vectors_vals) plot_scaling(n_vectors_vals, times) . 100%|██████████| 5/5 [01:10&lt;00:00, 14.12s/it] . Sanity check: compare to pure sklearn function . n_vectors = 30_000 n_dimensions = 300 all_vectors = np.random.random(size=(n_vectors, n_dimensions)) distances_skl = cosine_distances(all_vectors) distances_chunk = cosine_distance_chunks(all_vectors) np.allclose(distances_skl, distances_chunk) . True . . Tip: pairwise_distances_chunked has some parameters that can be pretty useful: - n_jobs: distribute the computation across cores (though you might want to experiment a bit since overhead might make it actually worse). - metric: choose a metric different from cosine distance[1], such as euclidean distance or even your own defined function. You can check the rest of them in the documentation. [1]: The sharp eye might have noticed that the term &quot;metric&quot; is not quite correct here. Strictly speaking cosine distance is not a metric (the reason for that can be found here). . Take home message: Whenever you find yourself carrying out an data/machine learning task and you have the feeling that there must be a better way, check scikit-learn first. The odds that you&#39;ll find something useful are really on your side. . Fin . References: . xkcd comic - extrapolation | . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/scikit-learn/scientific-computing/2019/11/30/divide-and-conquer.html",
            "relUrl": "/python/scikit-learn/scientific-computing/2019/11/30/divide-and-conquer.html",
            "date": " • Nov 30, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Bingo Percolation",
            "content": "Let’s play bingo percolation! . Disclaimer: what you’re about to read might be somehow trivial and analytically solvable after a bit of combinatorics-probabilities gymnastics. If that already sounds boring, imagine actually going through it with pen and paper. Let’s see what Python can do for us. . edit: I just learned about this post. Vincent approaches it from a complementary, analytical perspective, so check it out. . Disclaimer 2: this is not about coffee. I’m sorry too. . The story . So one day we where playing champagne-bingo because.. why not? It’s a pretty straight forward game. If you know bingo, it’s the same, but! everyone has a glass of champagne and is supposed to take a sip if the drawn number is not on his/her ticket. Easy. You can do the math for yourself about the number of times you end up having to drink..but that’s not the point here. . Percolation . Percolation refers to a whole world of interesting stuff, ranging from coffee brewing to epidemics spreading and everything in between. But it suffices here to know that the deal is about trying to understand phase transitions in a particular system. That means, characterizing and being able to predict a qualitative change in the behaviour of some macroscopic property of the system. For example, let’s say we have a bunch of isolated new users of a new social network. Now we start connecting them randomly, like getting them to be friends but without any criterion rather than a certain fixed probability of getting connected. Percolation theory helps us predicting what is the minimum amount of friends per person that we need (on average) in order to have a social network that is connected, i.e., in which everybody is reachable by taking a finite number of hops in the network. Pretty useful, right? . Back to the game . It turns out we did not stop playing when we where supposed to, i.e., whenever someone won (got bingo) just for the sake of.. you know.. just having fun a bit longer. . And my observation was that, after a long period of people doing little more than drinking number after number (pretty hard life), a rather abrupt transition seemed to occur and folks were getting (first) lines (and then) bingo more and more often. In other words, as time passed (numbers got drawn), the probability that someone would jump up, scream out loud “line” or “bingo” while performing a nice hula dance seemed to be higher and higher. Yes, you got it, a non-linear transition. Yes, like a percolation. To be more precise, a bingo-percolation! . So let’s see if that is actually the case. . Bingo . There are a couple of variations to it, but the one we played looked like this: . 75 balls numbered 1 to 75 | each ticket has 25 numbers (5x5 grid), each column having the numbers: (B) first: 1-15 | (I) second: 16-30 | (N) third: 31-45 | (G) fourth: 46-60 | (O) fifth: 61-75 | . | “line”: full horizontal/vertical/diagonal line marked * | “bingo”: full ticket marked | . (Minutia: For the “lines” we only track the occurrence of at least one line per ticket, since there was a price for only the first one getting it. So after a player gets a line, she will afterwards always count as simply having at least one line, independently from the fact that she can get a different line in the future.) . On to the code! . Requirements . numpy | matplotlib | seaborn | . # We need # - tickets # - number drawer # - ticket marker # - ticket checker # - game runner def make_tickets(n_tickets=10): &#39;&#39;&#39; Return 3D array - stack of n_tickets. &#39;&#39;&#39; tickets = [ np.transpose([ sorted(np.random.choice(row, size=5, replace=False)) for row in np.arange(1, 76).reshape(5, 15)]) for _ in range(n_tickets) ] if n_tickets == 1: return np.array(tickets)[0] return np.array(tickets) def draw_allnumbers(): &#39;&#39;&#39;Draw the numbers for the whole game at once&#39;&#39;&#39; return np.random.choice(range(1, 76), size=75, replace=False) def mark_ticket(ticket, number): &#39;&#39;&#39; Modify ticket putting a mark (0) if number is on it. &#39;&#39;&#39; ticket[ticket == number] = 0 return ticket def check_line(ticket): &#39;&#39;&#39; Return: (bool) ticket has at least one of the valid lines marked. &#39;&#39;&#39; line = any([ sum([np.all(row == 0) for row in ticket]) &gt; 0, # horizontal sum([np.all(row == 0) for row in ticket.T]) &gt; 0, # vertical np.all(np.diag(ticket) == 0), # diagonals np.all(np.diag(np.fliplr(ticket)) == 0) ]) return line def check_bingo(ticket): &#39;&#39;&#39;Return True if ticket has bingo (all numbers are marked)&#39;&#39;&#39; return np.all(ticket == 0) def play_game(n_players=100): # Initialize game tickets = make_tickets(n_players) numbers = draw_allnumbers() # Track number of lines/bingos after each number drawn lines = np.zeros_like(numbers) bingos = np.zeros_like(numbers) # Play rounds, number by number for i, number in enumerate(numbers): for ticket in tickets: mark_ticket(ticket, number) # Check how many players got have at least one line or bingo lines[i] += check_line(ticket) bingos[i] += check_bingo(ticket) return lines, bingos . That’s it! . So we can run the script bingo.py . python bingo.py . The output figure shows the results of 100 rounds of the game, played by 100 players. Individual traces correspond to each run, while the bold lines depict the average trace for each case. . . In fact, we see a sharp transition both in the proportion of players with at least one line as well as in the proportion of players with bingo. . Lo and behold, the curves kind of agree with the initial intuition :) . All the code can be found here. . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "relUrl": "/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "date": " • Oct 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": ". Hi, I’m Fabrizio. Thanks for your visit! . I am currently a PhD Student at the Institute of Computational Neuroscience in Hamburg (Germany). . Self-taught coder, strong open-source and team work advocate. I like music a lot, Python, berries, reading, simple ideas behind complex phenomena and data-driven solutions. I prefer strong intuitions and simple words to unnecessary sophistication and difficult jargon. . Feel free to get in touch on twitter, LinkedIn or just drop me an e-mail.1 . . This blog is powered by fastpages. &#8617; . |",
          "url": "https://fabridamicelli.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "code",
          "content": "More details coming soon. For the time being, you can check out my repo here. .",
          "url": "https://fabridamicelli.github.io/blog/code/",
          "relUrl": "/code/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "links",
          "content": "Definitely incomplete, always growing list of cool, free learning/fun resources out there. . Tutorials / Talks . calmcode: a goldmine for Python people – learn calmly bit by bit with hands-on tutorials. | Real Python: awesome tutorials about all sorts of aspects of the language. | Scipy Lectures: learn numerics, science, and data with Python. | Facts and Myths about Python names and values - Ned Batchelder - PyCon 2015 | Numba - Tell Those C++ Bullies to Get Lost - Forsyth &amp; Barba - SciPy 2017 | Reproducible workflow in Jupyter - Jake van der Plas | Statistics for Hackers - Jake van der Plas - PyCon 2016 | Computational Statistics - Allen Downey - SciPy 2017 | Anatomy of Matplotlib - Root &amp; Aizenman - SciPy 2018 | Untitled12 - Vincent Warmerdam - PyData 2019 | Gaussian Progress – Vincent Wamerdam - PyData 2019 | A Bluffer’s Guide to Dimension Reduction - Leland McInnes - PyData 2018 | fast.ai: a myriad of learning resources (ML, python, dev. skills, etc.) | fastpages: great tool to easily build blogs like this. | Setosa: visual explanations | Docker: nail down the basics with this tutorial from freeCodeCamp. | Git: go beyond typing commmands blindly - understand git’s data model. | . Youtube channels . 3brown1blue: one of the best teachers in the internet. Ever. | Corey Schafer: hands down one of the best Python resources out there. | anthonywritescode: mostly (not only) Python features, tools &amp; idioms explained. | Real Python | Luis serrano: machine learning algorithms and intuitions, in simple words. | Luke Smith: unix, vim, latex, markdown, free-software, etc. | Python Engineer | Ben Lambert: stats, ML, Bayesian stuff. | Statsquest – Josh Starmer: stats, machine learning, algorithms, bam! | Arxiv Insights: machine learning papers explained and discussed. | . Blogs . Vincent Warmerdam: data, machine learning, algorithms – and nice drawings. | Max Halford | Squirrel, Not Squirrel: amazing PCA explanation. | Michael Nielsen | . Podcasts . Python &amp; co. . Podcast.init | Talk python to me | Python Bytes | dats’n’stats | The Real Python | . Data, ML, Maths, etc. . Practical AI | AI in action | Machine Learning Street Talks | Complexity - Sta. Fe Institute | This week in machine learning | Underrated ML | Talking Machines | Numberphile | Data skeptic | Linear Digressions | Gradient dissent: Weights and biases | Dataframed | Partially derivative | . Discussion, interviews, philosophy . Sean Carrol’s Mindscape | The knowledge project | Not so standard deviations | Artificial Intelligence - Lex Fridman | Adversarial learning | Brain inspired | Stephen Wolfram podcast | Chai Tea Data Science | DeepMind Podcast | Towards data science | The data exchange | .",
          "url": "https://fabridamicelli.github.io/blog/links/",
          "relUrl": "/links/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "research",
          "content": "Short . Our group is interested in fundamental principles of brain connectivity and how they sculpt brain function. By means of computational analysis and modeling, we try to understand how the connectivity of brain networks comes about and underlies higher level properties. My research topic is about modeling how some typical topological features of brain networks emerge. . Not-so-short . The conceptual framework is a branch of Network Science, namely Network Neuroscience. Network science looks at complex systems from a particular point of view, taking advantage of some mathematical tools coming from graph theory to represent the complex system at hand. . Why networks? Because they are useful abstractions. The most fundamental reason for that is that networks are all about relationships between elements, captured as links between elements of the network (nodes). In fact, the minimal description of a network is a list of links. . We can thus think of the brain (or a part of it) as say dots (neurons, regions) connected by lines (dendrites, axons), i.e., a network. Once we have that representation we can describe what we see, for example, how many areas have a given number of links (a.k.a. degree distribution). People have carried out experiments and described brain connectivity of several species (some of that research belonging to a field known as connectomics). . When we look at those networks derived from connectivity data we find that some properties appear almost all the time. Of course, there are many nuances in the data, the analyses and interpretations that make the general principles less straight forward. Nevertheless, some principles of brain connectivity are pretty characteristic (although not necessarily exclusive of brains). For example, the degree distribution referred above tends to be very heterogeneous, with a few regions having disproportionately many links while the vast majority of regions present a much smaller number of connections. Our work focuses on trying to understand why that is the case. In other words, figuring out potential mechanisms that could explain how those brain networks get shaped into the connectivity patterns observed in nature. Check out the following publications if you’re interested in more details: . Topological reinforcement as a principle of modularity emergence in brain networks. Damicelli, Hilgetag, Hütt, Messé. Network Neuroscience, 2019. | Modular topology emerges from plasticity in a minimalistic excitable network model. Damicelli, Hilgetag, Hütt, Messé. Chaos, 2017. | .",
          "url": "https://fabridamicelli.github.io/blog/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fabridamicelli.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}