{
  
    
        "post0": {
            "title": "Python Gotchas",
            "content": ". Numpy . Doing array *= something is very different from array = array * something . TL; DR: Only use the form array *= something if you&#39;re 100% sure you are doing the right thing, otherwise, just go for array = array * something. . Let&#39;s see why. We define two functions that to the eyes of many (including past me) do just the same. . import numpy as np def multiply(array, scalar): array *= scalar # &lt;-- handy short hand, right? ;) return array def multiply2(array, scalar): array = array * scalar return array . Let&#39;s see them in action . a = np.arange(10.) # dot casts to float to avoid type errors b = np.arange(10.) . a . array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) . b . array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) . multiply(a, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . multiply(a, 2) . array([ 0., 4., 8., 12., 16., 20., 24., 28., 32., 36.]) . Hey, wait! What&#39;s going on? . a . array([ 0., 4., 8., 12., 16., 20., 24., 28., 32., 36.]) . . Warning: The operation modifies the array in place. . Let&#39;s see what the other version of our function does. . multiply2(b, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . multiply2(b, 2) . array([ 0., 2., 4., 6., 8., 10., 12., 14., 16., 18.]) . This time the input array stays the same, ie., the modification remained in the scope of the function. . Despite it being very basic, it is actually more difficult to debug than for the toy example in real life cases. For instance, in the middle of a long data preprocessing pipeline. If you load your data once and run the preprocessing pipeline once, you will probably not notice the bug (that&#39;s the tricky thing!). But if the loaded data are passed more than once through the pipeline (without reloading the whole data), each pass will be actually feeding different input. For example, if you run K-Fold cross-validation, most likely it won&#39;t crash or anything, but you will be passing K different datasets to your model and your validation will be just rubbish! . Conclusion: you&#39;d better be really sure of what you&#39;re doing with array = array * something. . References . xkcd comics | . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/numpy/2020/05/10/python-gotchas.html",
            "relUrl": "/python/numpy/2020/05/10/python-gotchas.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Does your embedding make sense?",
            "content": "It&#39;s not about the projections for the rest of 2020, I promise. Nor 2021. . TL;DR: . Does your embedding make any (human) sense? Take a *quick, interactive look* at its labels (if you have) &amp; compare it w/clusters.Thanks to great, idiomatic Altair! @jakevdp @vega_vis -Code: https://t.co/vpWRU6fs6H-Example: https://t.co/67F7AuDoNr@PyData #Python @python_tip pic.twitter.com/PIeDuNZ2gf . &mdash; Fabrizio Damicelli (@fabridamicelli) December 12, 2019 . Imagine you are working with high-dimensional data, that is, the position of each data point in that multidimensional space can be represented by a large number of other features/coordinates. For example, you measure a bunch of properties of a product where each item has some values associated, say, size, cost of production, price, CO2 footprint, etc. It could also be the case that your features are more abstract quantities. Instead of price or size, you could just have a bunch of numbers representing each item that don&#39;t necessarily have a human meaning, for instance a vector like this [0.11, 0.34, 0.15, 0.9]. Hopefully, those number actually mean something (whatever we understand by &quot;meaning something&quot;). Hopefully. Things start to get trickier. A beautiful example of that are word-embeddings, which is the thing I was originally playing with when I needed the visualization that I will show you. These embeddings represent each word with a vector, typically having between 300 and 1000 dimensions. That&#39;s what I mean by high-dimensional. What the coordinates actually mean in that case is highly non-trivial and you can find out about it in the above-mentioned article. Other datasets are even more massive in terms of the number of features though, e.g., gene expression data are usually orders of magnitude larger. . I&#39;d be saying nothing new if I pointed out that humans (we typically don&#39;t talk about that with other animals) have a hard time visualizing space with more than three dimensions. Even with 86 billion of neurons between their ears. At this point, several of you will be saying: &quot;but! Carl Sagan... bla, bla&quot;.. In case you&#39;re too young, too old or simple had something else better to do and didn&#39;t watch that episode, just watch this short video and then get back – you won&#39;t regreat: . . Back to the data. People do all sorts of tricks to wrap their mind around those wild, high-dimensional data in spaces with really funky shapes and fundamentally counterintuitive to our daily spatial perception of the world. Here comes a whole zoo of dimensionality reduction methods (e.g., PCA, tSNE) that project your data down to something your visual system can deal with. Of course, visualization is just one use case, but you may also want to store your data more efficiently or find outliers, or simply make life easier to your machine learning algorithm. We won&#39;t talk about those here. . Why bother? The good, the bad and the ugly metric. . Let&#39;s say you have your projected data in 2D, for example, after tSNE. First things first: you might want to check if the projection that your method just spit out makes any sense at all. But, why, what can go wrong? Well, a lot. For instance, your dimensionality reduction method might be using an inappropiate metric. In other words, the algorithm could be using a notion of distance between points in the multidimensional space which does not capture some fundamental aspect of the funky space that those data points live in. Thus, further operations we do with/to our data might be strongly misleading. That is not what we want. To keep it concrete, just take a look at these nice examples from the documentation of the UMAP library, where the effect of using different metrics is clearly depicted. . Some libraries, like UMAP, also allow you to document your embedding or plot interactive figures. But you might have a tougher luck this time around and just get a cloud of unintelligible points scattered when you plot the 2D-projected data. In that case, you might find it useful to interact with it by looking at the labels of your data. The labels can be anything, like a priori known categories the result of your favourite clustering method. . That&#39;s what you came for :) So without further ado, on to the code. . #collapse-hide import altair as alt import pandas as pd from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.cluster import KMeans def plot_interactive_embedding( source=None, x=None, y=None, target_col=None, color_col=None, alpha=0.9, markersize=40, grid=True, max_n_show = 25, figsize=(500, 500), filename=None ): &quot;&quot;&quot; Simple function for interactive visualization labels of a 2D embedding (e.g., PCA projection). Altair Chart is generated (as html), where one can brush over the scatter plots and given labels are shown. Color can be optionally mapped to values as well (e.g., to compare embedding with a clustering method). This is a quick adaptation of this example: https://altair-viz.github.io/gallery/scatter_linked_table.html Parameters - source: pandas Dataframe Data to plot. x: str Column name of x coordinate data. This name will be also used as axis label. y: str Column name of y coordinate data. This name will be also used as axis label. target_col: str Column name of target data, i.e., the labels to brush over. color_col: str, optional. Default None. Column name of data encoding color. If None, all points will have same color. alpha: float (0-1), optional. Default .9. Opacity of points. markersize: float, int, optional. Default 40. Size of the points. grid: bool, optional. Default True. Grid in the background. Set to False to remove it. max_n_show: int, optional. Dafault 25. Maximum number of (target) labels to show when brushing over the points. figsize: tuple (floats), optional. Default (500, 500). Values for (width, height) filename: str, optional. Default None. If given, the chart will be saved. The name must include extension - one of [.json, .html, .svg, .png]. Returns - chart: Altair Chart Instance of chart for further tweaking &quot;&quot;&quot; width, height = figsize # Brush for selection brush = alt.selection(type=&#39;interval&#39;) # Scatter Plot points = alt.Chart( source, width=width, height=height ).mark_circle(size=markersize).encode( x=x, y=y, color=alt.value(&#39;steelblue&#39;) if color_col is None else alt.Color(color_col+&quot;:N&quot;, scale=alt.Scale(scheme=&#39;Spectral&#39;)) ).add_selection(brush) # Base chart for data tables ranked_text = alt.Chart(source).mark_text().encode( y=alt.Y(&#39;row_number:O&#39;,axis=None) ).transform_window( row_number=&#39;row_number()&#39; ).transform_filter( brush ).transform_window( rank=&#39;rank(row_number)&#39; ).transform_filter( alt.datum.rank &lt; max_n_show ) # Data Tables text = ranked_text.encode(text=target_col+&quot;:N&quot;).properties(title=target_col) chart = alt.hconcat( points, text, ).configure_axis(grid=grid) if filename: chart.save(filename) return chart . . #collapse-show # Get digits dataset digits = load_digits() # Cluster it with kmeans and get the predicted labels kmlabel = KMeans(n_clusters=10, n_jobs=-1).fit_predict(digits.data) # Embed in 2D with tSNE embedding = TSNE(n_components=2, n_iter=500).fit_transform(digits.data) # Turn data into a dataframe digits_df = pd.DataFrame({ &quot;tSNE-1&quot;: embedding[:, 0], &quot;tSNE-2&quot;: embedding[:, 1], &quot;digit&quot;: digits.target, &quot;KM-label&quot;: kmlabel }) # Make the chart plot_interactive_embedding( source=digits_df, x=&quot;tSNE-1&quot;, y=&quot;tSNE-2&quot;, target_col=&quot;digit&quot;, color_col=&quot;KM-label&quot;, figsize=(450, 450), filename=&quot;your_filename&quot; ) . . You can save the output by passing a filename argument. That will generate an interactive file like this. . All the code can be found here. . A couple of links worth checking out: . Altair: &quot;Declarative Visualization in Python&quot;. This does the job under the hood of what I showed you. I did not even scratch the surface of that powerful, neat library. | This and this for cool visualizations of UMAP projections. | whatlies: &quot;A library that tries help you to understand. What lies in word embeddings?&quot;. This didn&#39;t exist when I wrote this function – would be my first choice now. It goes way beyond of what I showed and has integration with other NLP tools as well. | . Bonus: Funny games in high dimensions! . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "relUrl": "/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "date": " • Dec 12, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Bingo Percolation",
            "content": "Let’s play bingo percolation! . Disclaimer: what you’re about to read might be somehow trivial and analytically solvable after a bit of combinatorics-probabilities gymnastics. If that already sounds boring, imagine actually going through it with pen and paper. Let’s see what Python can do for us. . edit: I just learned about this post. Vincent approaches it from a complementary, analytical perspective, so check it out. . Disclaimer 2: this is not about coffee. I’m sorry too. . The story . So one day we where playing champagne-bingo because.. why not? It’s a pretty straight forward game. If you know bingo, it’s the same, but! everyone has a glass of champagne and is supposed to take a sip if the drawn number is not on his/her ticket. Easy. You can do the math for yourself about the number of times you end up having to drink..but that’s not the point here. . Percolation . Percolation refers to a whole world of interesting stuff, ranging from coffee brewing to epidemics spreading and everything in between. But it suffices here to know that the deal is about trying to understand phase transitions in a particular system. That means, characterizing and being able to predict a qualitative change in the behaviour of some macroscopic property of the system. For example, let’s say we have a bunch of isolated new users of a new social network. Now we start connecting them randomly, like getting them to be friends but without any criterion rather than a certain fixed probability of getting connected. Percolation theory helps us predicting what is the minimum amount of friends per person that we need (on average) in order to have a social network that is connected, i.e., in which everybody is reachable by taking a finite number of hops in the network. Pretty useful, right? . Back to the game . It turns out we did not stop playing when we where supposed to, i.e., whenever someone won (got bingo) just for the sake of.. you know.. just having fun a bit longer. . And my observation was that, after a long period of people doing little more than drinking number after number (pretty hard life), a rather abrupt transition seemed to occur and folks were getting (first) lines (and then) bingo more and more often. In other words, as time passed (numbers got drawn), the probability that someone would jump up, scream out loud “line” or “bingo” while performing a nice hula dance seemed to be higher and higher. Yes, you got it, a non-linear transition. Yes, like a percolation. To be more precise, a bingo-percolation! . So let’s see if that is actually the case. . Bingo . There are a couple of variations to it, but the one we played looked like this: . 75 balls numbered 1 to 75 | each ticket has 25 numbers (5x5 grid), each column having the numbers: (B) first: 1-15 | (I) second: 16-30 | (N) third: 31-45 | (G) fourth: 46-60 | (O) fifth: 61-75 | . | “line”: full horizontal/vertical/diagonal line marked * | “bingo”: full ticket marked | . (Minutia: For the “lines” we only track the occurrence of at least one line per ticket, since there was a price for only the first one getting it. So after a player gets a line, she will afterwards always count as simply having at least one line, independently from the fact that she can get a different line in the future.) . On to the code! . Requirements . numpy | matplotlib | seaborn | . # We need # - tickets # - number drawer # - ticket marker # - ticket checker # - game runner def make_tickets(n_tickets=10): &#39;&#39;&#39; Return 3D array - stack of n_tickets. &#39;&#39;&#39; tickets = [ np.transpose([ sorted(np.random.choice(row, size=5, replace=False)) for row in np.arange(1, 76).reshape(5, 15)]) for _ in range(n_tickets) ] if n_tickets == 1: return np.array(tickets)[0] return np.array(tickets) def draw_allnumbers(): &#39;&#39;&#39;Draw the numbers for the whole game at once&#39;&#39;&#39; return np.random.choice(range(1, 76), size=75, replace=False) def mark_ticket(ticket, number): &#39;&#39;&#39; Modify ticket putting a mark (0) if number is on it. &#39;&#39;&#39; ticket[ticket == number] = 0 return ticket def check_line(ticket): &#39;&#39;&#39; Return: (bool) ticket has at least one of the valid lines marked. &#39;&#39;&#39; line = any([ sum([np.all(row == 0) for row in ticket]) &gt; 0, # horizontal sum([np.all(row == 0) for row in ticket.T]) &gt; 0, # vertical np.all(np.diag(ticket) == 0), # diagonals np.all(np.diag(np.fliplr(ticket)) == 0) ]) return line def check_bingo(ticket): &#39;&#39;&#39;Return True if ticket has bingo (all numbers are marked)&#39;&#39;&#39; return np.all(ticket == 0) def play_game(n_players=100): # Initialize game tickets = make_tickets(n_players) numbers = draw_allnumbers() # Track number of lines/bingos after each number drawn lines = np.zeros_like(numbers) bingos = np.zeros_like(numbers) # Play rounds, number by number for i, number in enumerate(numbers): for ticket in tickets: mark_ticket(ticket, number) # Check how many players got have at least one line or bingo lines[i] += check_line(ticket) bingos[i] += check_bingo(ticket) return lines, bingos . That’s it! . So we can run the script bingo.py . python bingo.py . The output figure shows the results of 100 rounds of the game, played by 100 players. Individual traces correspond to each run, while the bold lines depict the average trace for each case. . . In fact, we see a sharp transition both in the proportion of players with at least one line as well as in the proportion of players with bingo. . Lo and behold, the curves kind of agree with the initial intuition :) . All the code can be found here. . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "relUrl": "/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "date": " • Oct 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "Hi, I’m Fabrizio. Thanks for your visit! . I am currently a PhD Student at the Institute of Computational Neuroscience in Hamburg (Germany) and Claus Hilgetag is my supervisor. . Self-taugh coder, strong open-source and team-work and advocate. I like music a lot, Python, berries and simple ideas behind complex phenomena. I prefer strong intuitions and simple words to unnecessary sophistication and difficult jargon. . Feel free to get in touch on twitter @fabridamicelli or drop me an e-mail f.{mylastname}@uke.de. . Research . Short . Our group is interested in fundamental principles of brain connectivity and how they sculpt brain function. By means of computational analysis and modeling, we try to understand how the connectivity of brain networks comes about and underlies higher level properties. My research topic is about modeling how some features of brain networks emerge. In particular, I try to model how different plasticity mechanisms shape some topological features of brain networks. . Not-so-short . The conceptual framework is a branch of Network Science, namely Network Neuroscience. Network science looks at complex systems from a particular point of view, taking advantage of some mathematical tools coming from graph theory to represent the complex system at hand. . Why networks? Because they are useful abstractions. The most fundamental reason for that is that networks are all about relationships between elements, captured as links between elements of the network (nodes). In fact, the minimal description of a network is a list of links. . We can thus think of the brain (or a part of it) as say dots (neurons, regions) connected by lines (dendrites, axons), i.e., a network. Once we have that representation we can describe what we see, for example, how many areas have a given number of links (a.k.a. degree distribution). People have carried out experiments and described brain connectivity of several species (some of that research belonging to a field known as connectomics). . When we look at those networks derived from connectivity data we find that some properties appear almost all the time. Of course, there are many nuances in the data, the analyses and interpretations that make the general principles less straight forward. Nevertheless, some principles of brain connectivity are pretty characteristic (although not necessarily exclusive of brains). For example, the degree distribution referred above tends to be very heterogeneous, with a few regions having disproportionately many links while the vast majority of regions present a much smaller number of connections. Our work focuses on trying to understand why that is the case. In other words, figuring out potential mechanisms that could explain how those brain networks get shaped into the connectivity patterns observed in nature. Check out the following publications if you’re interested in more details: . Topological reinforcement as a principle of modularity emergence in brain networks. Damicelli, Hilgetag, Hütt, Messé. Network Neuroscience, 2019. | Modular topology emerges from plasticity in a minimalistic excitable network model. Damicelli, Hilgetag, Hütt, Messé. Chaos, 2017. | .",
          "url": "https://fabridamicelli.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "career",
          "content": "Coming very, very soon! .",
          "url": "https://fabridamicelli.github.io/blog/career/",
          "relUrl": "/career/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "code",
          "content": "More details coming soon. For the time being, you can check out my repo here. .",
          "url": "https://fabridamicelli.github.io/blog/code/",
          "relUrl": "/code/",
          "date": ""
      }
      
  

  

  
      ,"page5": {
          "title": "links",
          "content": "A list of cool resources out there – coming soon! .",
          "url": "https://fabridamicelli.github.io/blog/links/",
          "relUrl": "/links/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "research",
          "content": "More details coming soon. For the time being, you can check out what people in our team do here and the following publications: . Topological reinforcement as a principle of modularity emergence in brain networks. Damicelli, Hilgetag, Hütt, Messé. Network Neuroscience, 2019. | Modular topology emerges from plasticity in a minimalistic excitable network model. Damicelli, Hilgetag, Hütt, Messé. Chaos, 2017. | .",
          "url": "https://fabridamicelli.github.io/blog/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fabridamicelli.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}