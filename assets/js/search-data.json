{
  
    
        "post0": {
            "title": "Does your embedding make sense?",
            "content": "It&#39;s not about the projections for the rest of 2020, I promise. Nor 2021. . Imagine you are working with high-dimensional data, that is, the position of each data point in that multidimensional space can be represented by a large number of other features/coordinates. For example, you measure a bunch of properties of a product where each item has some values associated, say, size, cost of production, price, CO2 footprint, etc. It could also be the case that your features are more abstract quantities. Instead of price or size, you could just have a bunch of numbers representing each item that don&#39;t necessarily have a human meaning, for instance a vector like this [0.11, 0.34, 0.15, 0.9]. Hopefully, those number actually mean something (whatever we understand by &quot;meaning something&quot;). Hopefully. Things start to get trickier. A beautiful example of that are word-embeddings, which is the thing I was originally playing with when I needed the visualization that I will show you. These embeddings represent each word with a vector, typically of length between 300 and 1000. That&#39;s what I mean high-dimensional. What the coordinates actually mean in that case is highly non-trivial and you can find out about it in the article linked above. Other datasets are even more massive in terms of the number of features though, e.g., gene expression data are usually orders of magnitude larger. . I&#39;d be saying nothing new if I pointed out that humans (we typically don&#39;t talk about that with other animals) have a hard time visualizing space with more than three dimensions. Even with 86 billion of neurons between their ears. At this point, several of you will be saying: &quot;but! Carl Sagan... bla, bla&quot;.. In case you&#39;re too young, too old or simple had something else better to do and didn&#39;t watch that episode, just this short video and then get back – you won&#39;t regreat: . . Back to your data. People do all sorts of tricks to wrap their mind around those wild, high-dimensional data, living in spaces with really funky shapes and fundamentally counterintuitive to our daily spatial perception of the world. Here comes a whole zoo of dimensionality reduction methods (e.g., PCA, tSNE) that project your data down to something your visual system can deal with. Of course, visualization is just one use case, but you may also want to store your data more efficiently or find outliers, or simply make life easier to your machine learning algorithm. We won&#39;t talk about those here. . Why bother? The good, the bad and the ugly metric. . Let&#39;s say you have your projected data in 2D, for example, after PCA. First things first: you might want to check if the projection that your method just spit out makes any sense at all. But, why, what can go wrong? Well, a lot. For instance, your dimensionality reduction method might be using an inappropiate metric. In other words, the algorithm could be using a notion of distance between points in the multidimensional space which does not capture some fundamental aspect of the funky space that those data points live in. Thus, further operations we do with/to our data might be strongly misleading. That is not what we want. To keep it concrete, just take a look at these nice examples from the documentation of the UMAP library, where the effect of using different metrics is clearly depicted. . Some libraries, like UMAP, also allow you to document your embedding or plot interactive figures. But you might have a tougher luck this time around and just get a cloud of unintelligible points scattered when you plot the 2D-projected data. In that case, you might find it useful to interact with it by looking at the labels of your data. The labels can be anything, like known categories of your data or the result of your favourite clustering method. . That&#39;s what you came for :) So without further ado, on to the code. . #collapse-hide import pandas as pd from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.cluster import KMeans import altair as alt def plot_interactive_embedding( source=None, x=None, y=None, target_col=None, color_col=None, alpha=0.9, markersize=40, grid=True, max_n_show = 25, figsize=(500, 500), filename=None ): &quot;&quot;&quot; Simple function for interactive visualization labels of a 2D embedding (e.g., PCA projection). Altair Chart is generated (as html), where one can brush over the scatter plots and given labels are shown. Color can be optionally mapped to values as well (e.g., to compare embedding with a clustering method). This is a quick adaptation of this example: https://altair-viz.github.io/gallery/scatter_linked_table.html Parameters - source: pandas Dataframe Data to plot. x: str Column name of x coordinate data. This name will be also used as axis label. y: str Column name of y coordinate data. This name will be also used as axis label. target_col: str Column name of target data, i.e., the labels to brush over. color_col: str, optional. Default None. Column name of data encoding color. If None, all points will have same color. alpha: float (0-1), optional. Default .9. Opacity of points. markersize: float, int, optional. Default 40. Size of the points. grid: bool, optional. Default True. Grid in the background. Set to False to remove it. max_n_show: int, optional. Dafault 25. Maximum number of (target) labels to show when brushing over the points. figsize: tuple (floats), optional. Default (500, 500). Values for (width, height) filename: str, optional. Default None. If given, the chart will be saved. The name must include extension - one of [.json, .html, .svg, .png]. Returns - chart: Altair Chart Instance of chart for further tweaking &quot;&quot;&quot; width, height = figsize # Brush for selection brush = alt.selection(type=&#39;interval&#39;) # Scatter Plot points = alt.Chart( source, width=width, height=height ).mark_circle(size=markersize).encode( x=x, y=y, color=alt.value(&#39;steelblue&#39;) if color_col is None else alt.Color(color_col+&quot;:N&quot;, scale=alt.Scale(scheme=&#39;Spectral&#39;)) ).add_selection(brush) # Base chart for data tables ranked_text = alt.Chart(source).mark_text().encode( y=alt.Y(&#39;row_number:O&#39;,axis=None) ).transform_window( row_number=&#39;row_number()&#39; ).transform_filter( brush ).transform_window( rank=&#39;rank(row_number)&#39; ).transform_filter( alt.datum.rank &lt; max_n_show ) # Data Tables text = ranked_text.encode(text=target_col+&quot;:N&quot;).properties(title=target_col) chart = alt.hconcat( points, text, ).configure_axis(grid=grid) if filename: chart.save(filename) return chart . . #collapse-show import pandas as pd from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.cluster import KMeans import altair as alt source=digits_df x=&quot;tSNE-1&quot; y=&quot;tSNE-2&quot; target_col=&quot;digit&quot; color_col=&quot;KM-label&quot; alpha=0.9 markersize=40 grid=True max_n_show = 25 width, height = 500, 500 # Brush for selection brush = alt.selection(type=&#39;interval&#39;) # Scatter Plot points = alt.Chart( source, width=width, height=height ).mark_circle(size=markersize).encode( x=x, y=y, color=alt.value(&#39;steelblue&#39;) if color_col is None else alt.Color(color_col+&quot;:N&quot;, scale=alt.Scale(scheme=&#39;Spectral&#39;)) ).add_selection(brush) # Base chart for data tables ranked_text = alt.Chart(source).mark_text().encode( y=alt.Y(&#39;row_number:O&#39;,axis=None) ).transform_window( row_number=&#39;row_number()&#39; ).transform_filter( brush ).transform_window( rank=&#39;rank(row_number)&#39; ).transform_filter( alt.datum.rank &lt; max_n_show ) # Data Tables text = ranked_text.encode(text=target_col+&quot;:N&quot;).properties(title=target_col) alt.hconcat( points, text, ).configure_axis(grid=grid) . . #collapse-hide # Get digits dataset digits = load_digits() # Cluster it with kmeans and get the predicted labels kmlabel = KMeans(n_clusters=10, n_jobs=-1).fit_predict(digits.data) # Embed in 2D with tSNE embedding = TSNE(n_components=2, n_iter=500).fit_transform(digits.data) . . #collapse-hide # Turn data into a dataframe digits_df = pd.DataFrame({ &quot;tSNE-1&quot;: embedding[:, 0], &quot;tSNE-2&quot;: embedding[:, 1], &quot;digit&quot;: digits.target, &quot;KM-label&quot;: kmlabel }) . . #collapse-show # Make the chart chart = plot_interactive_embedding( source=digits_df, x=&quot;tSNE-1&quot;, y=&quot;tSNE-2&quot;, target_col=&quot;digit&quot;, color_col=&quot;KM-label&quot;, figsize=(450, 450), ) chart.display() . . All the code can be found here. . Before you leave, check out these: . Altair: &quot;Declarative Visualization in Python&quot;. This does the job under the hood of what I showed you. I did not even scratch the surface of that powerful, neat library. | This and this for cool visualizations of UMAP projections. | whatlies: &quot;A library that tries help you to understand. What lies in word embeddings?&quot;. This didn&#39;t exist when I wrote this function – would be my first choice now. It goes way beyond of what I showed and has integration with other NLP tools as well. | . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "relUrl": "/python/dataviz/dimensionality-reduction/2019/12/12/interactive-embedding.html",
            "date": " • Dec 12, 2019"
        }
        
    
  
    
        ,"post1": {
            "title": "Bingo Percolation",
            "content": "Let’s play bingo percolation! . Disclaimer: what you’re about to read might be somehow trivial and analytically solvable after a bit of combinatorics-probabilities gymnastics. If that already sounds boring, imagine actually going through it with pen and paper. Let’s see what Python can do for us. . edit: I just learned about this post. Vincent approaches it from a complementary, analytical perspective, so check it out. . Disclaimer 2: this is not about coffee. I’m sorry too. . The story . So one day we where playing champagne-bingo because.. why not? It’s a pretty straight forward game. If you know bingo, it’s the same, but! everyone has a glass of champagne and is supposed to take a sip if the drawn number is not on his/her ticket. Easy. You can do the math for yourself about the number of times you end up having to drink..but that’s not the point here. . Percolation . Percolation refers to a whole world of interesting stuff, ranging from coffee brewing to epidemics spreading and everything in between. But it suffices here to know that the deal is about trying to understand phase transitions in a particular system. That means, characterizing and being able to predict a qualitative change in the behaviour of some macroscopic property of the system. For example, let’s say we have a bunch of isolated new users of a new social network. Now we start connecting them randomly, like getting them to be friends but without any criterion rather than a certain fixed probability of getting connected. Percolation theory helps us predicting what is the minimum amount of friends per person that we need (on average) in order to have a social network that is connected, i.e., in which everybody is reachable by taking a finite number of hops in the network. Pretty useful, right? . Back to the game . It turns out we did not stop playing when we where supposed to, i.e., whenever someone won (got bingo) just for the sake of.. you know.. just having fun a bit longer. . And my observation was that, after a long period of people doing little more than drinking number after number (pretty hard life), a rather abrupt transition seemed to occur and folks were getting (first) lines (and then) bingo more and more often. In other words, as time passed (numbers got drawn), the probability that someone would jump up, scream out loud “line” or “bingo” while performing a nice hula dance seemed to be higher and higher. Yes, you got it, a non-linear transition. Yes, like a percolation. To be more precise, a bingo-percolation! . So let’s see if that is actually the case. . Bingo . There are a couple of variations to it, but the one we played looked like this: . 75 balls numbered 1 to 75 | each ticket has 25 numbers (5x5 grid), each column having the numbers: (B) first: 1-15 | (I) second: 16-30 | (N) third: 31-45 | (G) fourth: 46-60 | (O) fifth: 61-75 | . | “line”: full horizontal/vertical/diagonal line marked * | “bingo”: full ticket marked | . (Minutia: For the “lines” we only track the occurrence of at least one line per ticket, since there was a price for only the first one getting it. So after a player gets a line, she will afterwards always count as simply having at least one line, independently from the fact that she can get a different line in the future.) . On to the code! . Requirements . numpy | matplotlib | seaborn | . # We need # - tickets # - number drawer # - ticket marker # - ticket checker # - game runner def make_tickets(n_tickets=10): &#39;&#39;&#39; Return 3D array - stack of n_tickets. &#39;&#39;&#39; tickets = [ np.transpose([ sorted(np.random.choice(row, size=5, replace=False)) for row in np.arange(1, 76).reshape(5, 15)]) for _ in range(n_tickets) ] if n_tickets == 1: return np.array(tickets)[0] return np.array(tickets) def draw_allnumbers(): &#39;&#39;&#39;Draw the numbers for the whole game at once&#39;&#39;&#39; return np.random.choice(range(1, 76), size=75, replace=False) def mark_ticket(ticket, number): &#39;&#39;&#39; Modify ticket putting a mark (0) if number is on it. &#39;&#39;&#39; ticket[ticket == number] = 0 return ticket def check_line(ticket): &#39;&#39;&#39; Return: (bool) ticket has at least one of the valid lines marked. &#39;&#39;&#39; line = any([ sum([np.all(row == 0) for row in ticket]) &gt; 0, # horizontal sum([np.all(row == 0) for row in ticket.T]) &gt; 0, # vertical np.all(np.diag(ticket) == 0), # diagonals np.all(np.diag(np.fliplr(ticket)) == 0) ]) return line def check_bingo(ticket): &#39;&#39;&#39;Return True if ticket has bingo (all numbers are marked)&#39;&#39;&#39; return np.all(ticket == 0) def play_game(n_players=100): # Initialize game tickets = make_tickets(n_players) numbers = draw_allnumbers() # Track number of lines/bingos after each number drawn lines = np.zeros_like(numbers) bingos = np.zeros_like(numbers) # Play rounds, number by number for i, number in enumerate(numbers): for ticket in tickets: mark_ticket(ticket, number) # Check how many players got have at least one line or bingo lines[i] += check_line(ticket) bingos[i] += check_bingo(ticket) return lines, bingos . That’s it! . So we can run the script bingo.py . python bingo.py . The output figure shows the results of 100 rounds of the game, played by 100 players. Individual traces correspond to each run, while the bold lines depict the average trace for each case. . . In fact, we see a sharp transition both in the proportion of players with at least one line as well as in the proportion of players with bingo. . Lo and behold, the curves kind of agree with the initial intuition :) . All the code can be found here. . . Any bugs, questions, comments, suggestions? Ping me on twitter or drop me an e-mail. .",
            "url": "https://fabridamicelli.github.io/blog/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "relUrl": "/python/probabilities/computational-stats/2019/10/01/bingo-percolation.html",
            "date": " • Oct 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "Hi! I’m Fabrizio, a PhD Student at the Institute of Computational Neuroscience in Hamburg (Germany). Check out research and career to learn a bit more about that. . More details coming soon! .",
          "url": "https://fabridamicelli.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "career",
          "content": "Coming very, very soon! .",
          "url": "https://fabridamicelli.github.io/blog/career/",
          "relUrl": "/career/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "code",
          "content": "More details coming soon. For the time being, you can check out my repo here. .",
          "url": "https://fabridamicelli.github.io/blog/code/",
          "relUrl": "/code/",
          "date": ""
      }
      
  

  

  
      ,"page5": {
          "title": "links",
          "content": "A list of cool resources out there – coming soon! .",
          "url": "https://fabridamicelli.github.io/blog/links/",
          "relUrl": "/links/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "research",
          "content": "Coming very, very soon! .",
          "url": "https://fabridamicelli.github.io/blog/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fabridamicelli.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}